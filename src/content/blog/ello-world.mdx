---
title: "ello world"
date: 2024-11-16
excerpt: Lift off! Embarking on a journey with Astro! üë®‚ÄçüöÄ
tags: [astro]
---

Hey! First post is here, published using [Astro](https://astro.build/).

I will use this site as a devlog on my journey getting deeper into AI/ML.

My first interaction with machine learning was [`sklearn`](https://scikit-learn.org/stable/), which still has a fantastic and well designed API. I used it primarily for classification on tabular data. Think of a large spreadsheet with many columns, and you need to predict a column value based on the rest.

I opened the door to NLP in around 2017, when I discovered [Word2vec](https://en.wikipedia.org/wiki/Word2vec). Never managed to get off the ground until I discovered [SpaCy](https://spacy.io/) where I managed to create some meaningful classification and named entity recognition. Still requried lots of labelling!

Then came BERT and Hugging Face ü§ó was only a [neural coreference system](https://huggingface.co/coref/), but soon released [Write with Transformers](https://banana-projects-transformer-autocomplete.hf.space/) and the rest is history!

Computer Vision was often more effective on some tasks. For example classification of a dense document as being a certain type of form ([airworthiness approval forms](https://ecommerce.aviationeu.supplies/en/content/8-faaeasa-airworthiness-approval-forms-for-productsparts)) was more efficient with better accuracy as an image classification problem using AWS Rekognition at the time.

Applied similar technique to find page orientation problems in scanned documents:

<blockquote class="twitter-tweet">
  <a href="https://twitter.com/MartinHN/status/1440762582910963712"></a>
</blockquote>

Object detection was also useful with document images:

<blockquote class="twitter-tweet">
  <a href="https://twitter.com/MartinHN/status/976928114575134720"></a>
</blockquote>
<script
  async
  src="https://platform.twitter.com/widgets.js"
  charset="utf-8"
></script>

Today, large language models and moreso, Vision Language Models (VLMs) can doo all of this with few-shot prompting (if not zero-shot). For efficiency, use it to label examples and distil a smaller model, optimize it with quantization and ONNX runtime and you have a beast!
